{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import errno\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('./modified_medsam_repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = glob('/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/*.csv')\n",
    "for x in filelist:\n",
    "    fileid = os.path.basename(x).split('.csv')[0]\n",
    "    to_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/subsample_symlinks', fileid)\n",
    "    if not os.path.exists(os.path.join(to_path, 'train', 'images')):\n",
    "        os.makedirs(os.path.join(to_path, 'train', 'images'))\n",
    "        os.makedirs(os.path.join(to_path, 'train', 'labels'))\n",
    "\n",
    "    df_read = pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{fileid}.csv')\n",
    "    df_read = df_read.drop_duplicates(subset = ['id', 'slice'])\n",
    "    print(f'on {fileid}')\n",
    "    for idx, r in tqdm(df_read.iterrows(), total = df_read.shape[0]):\n",
    "        ending = f'{r.id}_slice{r.slice}.png'\n",
    "        from_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/train/images', ending)\n",
    "        to_path_im = os.path.join(to_path, 'train', 'images', ending)\n",
    "        try:\n",
    "            os.symlink(from_path, to_path_im)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                os.remove(to_path_im)\n",
    "                os.symlink(from_path, to_path_im)\n",
    "            else:\n",
    "                print('uncaught error')\n",
    "\n",
    "        # label\n",
    "        ending = f'{r.id}_slice{r.slice}.txt'\n",
    "        from_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/train/labels', ending)\n",
    "        to_path_la = os.path.join(to_path, 'train', 'labels', ending)\n",
    "        try:\n",
    "            os.symlink(from_path, to_path_la)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                os.remove(to_path_la)\n",
    "                os.symlink(from_path, to_path_la)\n",
    "            else:\n",
    "                print('uncaught error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78117/78117 [00:04<00:00, 15778.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 500: 78117\n",
      "on 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28133/28133 [00:01<00:00, 15050.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 100: 28133\n",
      "on 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109292/109292 [00:10<00:00, 10611.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 750: 109292\n",
      "on 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46825/46825 [00:06<00:00, 7683.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 250: 46825\n",
      "on 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21723/21723 [00:01<00:00, 12456.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 50: 21723\n",
      "on 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16121/16121 [00:00<00:00, 17423.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 5: 16121\n",
      "on 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16663/16663 [00:01<00:00, 14727.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 10: 16663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filelist = glob('/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/*.csv')\n",
    "\n",
    "\n",
    "for x in filelist:\n",
    "    listo_paths = []\n",
    "    fileid = os.path.basename(x).split('.csv')[0]\n",
    "    #to_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/subsample_symlinks', fileid)\n",
    "\n",
    "    df_read = pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{fileid}.csv')\n",
    "    df_read = df_read.drop_duplicates(subset = ['id', 'slice'])\n",
    "    print(f'on {fileid}')\n",
    "    for idx, r in tqdm(df_read.iterrows(), total = df_read.shape[0]):\n",
    "        ending = f'{r.id}_slice{r.slice}.png'\n",
    "        from_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/train/images', ending)\n",
    "        listo_paths.append(from_path)\n",
    "\n",
    "    with open(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/yolov7_subsample_txt_files/{fileid}.txt', 'w') as fp:\n",
    "        for item in listo_paths:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "        print(f'for {fileid}: {len(listo_paths)}')\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/50.csv')['id'].to_numpy()).shape[0] - 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16120\n",
      "16121\n",
      "===\n",
      "16662\n",
      "16663\n",
      "===\n",
      "21722\n",
      "21723\n",
      "===\n",
      "28132\n",
      "28133\n",
      "===\n",
      "46824\n",
      "46825\n",
      "===\n",
      "78116\n",
      "78117\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for ext in ['5', '10', '50', '100', '250', '500']:\n",
    "    print(pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/yolov7_subsample_txt_files/{ext}.txt').shape[0])\n",
    "    xd = pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{ext}.csv')\n",
    "    xd = xd.drop_duplicates(subset = ['id', 'slice'])\n",
    "    print(xd.shape[0])\n",
    "    #print(xd)\n",
    "    print('===')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
