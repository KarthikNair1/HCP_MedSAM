{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "join = os.path.join\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from monai.networks import one_hot\n",
    "import sys\n",
    "sys.path.append('./modified_medsam_repo')\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from utils.SurfaceDice import compute_dice_coefficient\n",
    "from skimage import io, transform\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "import random\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import img2pdf\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "from MedSAM_HCP.dataset import MRIDatasetForPooled, load_datasets\n",
    "from MedSAM_HCP.MedSAM import MedSAM, medsam_inference\n",
    "from MedSAM_HCP.build_sam import build_sam_vit_b_multiclass\n",
    "from MedSAM_HCP.utils_hcp import *\n",
    "from typing import List\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_ids(id_list: List[int], is_test=False):\n",
    "    assert len(id_list) == 1\n",
    "    path_df_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv' # gets all the files\n",
    "    df = pd.read_csv(path_df_path)\n",
    "    df = df[df['id'].isin(id_list)].reset_index(drop=True)\n",
    "    \n",
    "    if is_test:\n",
    "        path_for_bboxes = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/test/path_df_pooled_labels_only_with_bbox_yolov7_TEST.csv'\n",
    "    else:\n",
    "        path_for_bboxes = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/path_df_pooled_labels_only_with_bbox_yolov7.csv'\n",
    "    \n",
    "    df_bboxes = pd.read_csv(path_for_bboxes,index_col=0)\n",
    "    df_bboxes = df_bboxes.drop(columns = ['index'])\n",
    "    \n",
    "    df_bboxes = df_bboxes[df_bboxes['id'].isin(id_list)].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    df = df.drop(columns = ['bbox_0', 'bbox_1', 'bbox_2', 'bbox_3'])\n",
    "\n",
    "    #print(df.shape)\n",
    "    #print(df)\n",
    "    #print(df_bboxes.shape)\n",
    "    #print(df_bboxes)\n",
    "\n",
    "    df = df.merge(df_bboxes, how='left', on=['id','slice','image_embedding_slice_path', 'segmentation_slice_path', 'image_path'])\n",
    "    return df\n",
    "\n",
    "def df_to_dataset(df, as_one_hot=True, pool_labels=True):\n",
    "    df_hcp = pd.read_csv('/gpfs/home/kn2347/MedSAM/hcp_mapping_processed.csv')\n",
    "    df_desired = pd.read_csv('/gpfs/home/kn2347/MedSAM/darts_name_class_mapping_processed.csv')\n",
    "    NUM_CLASSES = len(df_desired)\n",
    "    label_converter = LabelConverter(df_hcp, df_desired)\n",
    "\n",
    "    dataset = MRIDatasetForPooled(df, 1, 0, label_converter = label_converter, NUM_CLASSES=NUM_CLASSES, as_one_hot=as_one_hot, pool_labels=pool_labels)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 5)\n",
      "         id  slice                         image_embedding_slice_path  \\\n",
      "0    162935      0  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "1    162935      1  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "2    162935      2  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3    162935      3  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "4    162935      4  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "..      ...    ...                                                ...   \n",
      "251  162935    251  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "252  162935    252  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "253  162935    253  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "254  162935    254  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "255  162935    255  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "\n",
      "                               segmentation_slice_path  \\\n",
      "0    /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "1    /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "2    /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3    /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "4    /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "..                                                 ...   \n",
      "251  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "252  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "253  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "254  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "255  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "\n",
      "                                            image_path  \n",
      "0    /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "1    /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "2    /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "3    /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "4    /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "..                                                 ...  \n",
      "251  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "252  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "253  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "254  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "255  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...  \n",
      "\n",
      "[256 rows x 5 columns]\n",
      "(3603, 10)\n",
      "          id  slice                         image_embedding_slice_path  \\\n",
      "0     162935     94  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "1     162935    134  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "2     162935    105  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3     162935    107  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "4     162935    125  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "...      ...    ...                                                ...   \n",
      "3598  162935    155  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3599  162935    137  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3600  162935    154  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3601  162935    152  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3602  162935    126  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "\n",
      "                                segmentation_slice_path  \\\n",
      "0     /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "1     /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "2     /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3     /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "4     /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "...                                                 ...   \n",
      "3598  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3599  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3600  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3601  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "3602  /gpfs/data/luilab/karthik/pediatric_seg_proj/h...   \n",
      "\n",
      "                                             image_path  bbox_0  bbox_1  \\\n",
      "0     /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...      77     132   \n",
      "1     /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...      52     126   \n",
      "2     /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...      66     131   \n",
      "3     /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...      66     131   \n",
      "4     /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...      61     129   \n",
      "...                                                 ...     ...     ...   \n",
      "3598  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...     136      86   \n",
      "3599  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...     118      83   \n",
      "3600  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...     135      86   \n",
      "3601  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...     130      84   \n",
      "3602  /gpfs/data/cbi/hcp/hcp_seg/data_orig/162935/mr...     124      89   \n",
      "\n",
      "      bbox_2  bbox_3  label_number  \n",
      "0        169     173             1  \n",
      "1        206     190             1  \n",
      "2        193     183             1  \n",
      "3        194     184             1  \n",
      "4        211     189             1  \n",
      "...      ...     ...           ...  \n",
      "3598     163     118           102  \n",
      "3599     170      99           102  \n",
      "3600     163     118           102  \n",
      "3601     164     101           102  \n",
      "3602     132      93           102  \n",
      "\n",
      "[3603 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#df = get_df_for_ids([162935], is_test=True)\n",
    "#dataset = df_to_dataset(df, as_one_hot=True, pool_labels=False)\n",
    "#dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [00:41<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done all\n",
      "torch.Size([16, 1, 256, 256])\n",
      "torch.Size([16])\n",
      "hello at 91.0 torch.Size([40])\n",
      "hello at 93.0 torch.Size([40])\n",
      "hello at 98.0 torch.Size([40])\n",
      "hello at 62.0 torch.Size([38])\n",
      "hello at 86.0 torch.Size([38])\n",
      "hello at 55.0 torch.Size([37])\n",
      "hello at 60.0 torch.Size([37])\n",
      "hello at 67.0 torch.Size([37])\n",
      "hello at 94.0 torch.Size([37])\n",
      "hello at 97.0 torch.Size([37])\n",
      "hello at 20.0 torch.Size([28])\n",
      "hello at 1.0 torch.Size([35])\n",
      "hello at 63.0 torch.Size([35])\n",
      "hello at 66.0 torch.Size([75])\n",
      "hello at 42.0 torch.Size([68])\n",
      "hello at 73.0 torch.Size([68])\n",
      "hello at 100.0 torch.Size([68])\n",
      "hello at 77.0 torch.Size([79])\n",
      "hello at 69.0 torch.Size([82])\n",
      "hello at 46.0 torch.Size([55])\n",
      "hello at 96.0 torch.Size([127])\n",
      "hello at 61.0 torch.Size([81])\n",
      "hello at 65.0 torch.Size([81])\n",
      "hello at 92.0 torch.Size([81])\n",
      "hello at 41.0 torch.Size([58])\n",
      "hello at 48.0 torch.Size([128])\n",
      "hello at 79.0 torch.Size([128])\n",
      "hello at 72.0 torch.Size([17])\n",
      "hello at 43.0 torch.Size([108])\n",
      "hello at 87.0 torch.Size([109])\n",
      "hello at 74.0 torch.Size([100])\n",
      "hello at 56.0 torch.Size([134])\n",
      "hello at 49.0 torch.Size([35])\n",
      "hello at 80.0 torch.Size([35])\n",
      "hello at 21.0 torch.Size([30])\n",
      "hello at 26.0 torch.Size([30])\n",
      "hello at 38.0 torch.Size([50])\n",
      "hello at 89.0 torch.Size([50])\n",
      "hello at 2.0 torch.Size([79])\n",
      "hello at 7.0 torch.Size([79])\n",
      "hello at 36.0 torch.Size([79])\n",
      "hello at 37.0 torch.Size([79])\n",
      "hello at 39.0 torch.Size([79])\n",
      "hello at 99.0 torch.Size([79])\n",
      "hello at 64.0 torch.Size([154])\n",
      "hello at 68.0 torch.Size([154])\n",
      "hello at 95.0 torch.Size([154])\n",
      "hello at 102.0 torch.Size([28])\n",
      "hello at 19.0 torch.Size([64])\n",
      "hello at 25.0 torch.Size([64])\n",
      "hello at 34.0 torch.Size([64])\n",
      "hello at 71.0 torch.Size([64])\n",
      "hello at 6.0 torch.Size([105])\n",
      "hello at 84.0 torch.Size([105])\n",
      "hello at 15.0 torch.Size([127])\n",
      "hello at 70.0 torch.Size([127])\n",
      "hello at 40.0 torch.Size([137])\n",
      "hello at 58.0 torch.Size([137])\n",
      "hello at 90.0 torch.Size([137])\n",
      "hello at 101.0 torch.Size([137])\n",
      "hello at 8.0 torch.Size([86])\n",
      "hello at 27.0 torch.Size([86])\n",
      "hello at 53.0 torch.Size([86])\n",
      "hello at 59.0 torch.Size([87])\n",
      "hello at 29.0 torch.Size([92])\n",
      "hello at 81.0 torch.Size([92])\n",
      "hello at 82.0 torch.Size([92])\n",
      "hello at 28.0 torch.Size([179])\n",
      "hello at 51.0 torch.Size([179])\n",
      "hello at 13.0 torch.Size([177])\n",
      "hello at 50.0 torch.Size([177])\n",
      "hello at 9.0 torch.Size([38])\n",
      "hello at 47.0 torch.Size([79])\n",
      "hello at 5.0 torch.Size([145])\n",
      "hello at 10.0 torch.Size([145])\n",
      "hello at 24.0 torch.Size([145])\n",
      "hello at 83.0 torch.Size([145])\n",
      "hello at 52.0 torch.Size([175])\n",
      "hello at 12.0 torch.Size([198])\n",
      "hello at 76.0 torch.Size([198])\n",
      "hello at 78.0 torch.Size([198])\n",
      "hello at 31.0 torch.Size([153])\n",
      "hello at 32.0 torch.Size([153])\n",
      "hello at 57.0 torch.Size([153])\n",
      "hello at 16.0 torch.Size([95])\n",
      "hello at 17.0 torch.Size([95])\n",
      "hello at 54.0 torch.Size([158])\n",
      "hello at 88.0 torch.Size([158])\n",
      "hello at 45.0 torch.Size([61])\n",
      "hello at 85.0 torch.Size([158])\n",
      "hello at 4.0 torch.Size([71])\n",
      "hello at 23.0 torch.Size([71])\n",
      "hello at 33.0 torch.Size([219])\n",
      "hello at 30.0 torch.Size([70])\n",
      "hello at 14.0 torch.Size([142])\n",
      "hello at 11.0 torch.Size([180])\n",
      "hello at 22.0 torch.Size([180])\n",
      "hello at 3.0 torch.Size([214])\n",
      "hello at 35.0 torch.Size([42])\n",
      "hello at 44.0 torch.Size([121])\n",
      "hello at 75.0 torch.Size([227])\n"
     ]
    }
   ],
   "source": [
    "# code to generate for pooled model\n",
    "def dataset_to_predictions_for_pooled_model(id_list, checkpoint, save_path, device='cuda', is_test=False):\n",
    "\n",
    "    df = get_df_for_ids(id_list, is_test=is_test)\n",
    "    dataset = df_to_dataset(df, as_one_hot=True, pool_labels=True)\n",
    "    dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size = 16,\n",
    "            shuffle = False,\n",
    "            num_workers = 0,\n",
    "            pin_memory = True)\n",
    "\n",
    "    model = build_sam_vit_b_multiclass(103, checkpoint=checkpoint).to(device)\n",
    "\n",
    "    collector = []\n",
    "    ctr = 0\n",
    "    overall_tensor = torch.zeros((256, 256, 256)).cuda()\n",
    "        \n",
    "    pred_collector = []\n",
    "    label_num_collector = []\n",
    "    slice_collector = []\n",
    "\n",
    "    viz = {}\n",
    "    print(len(dataloader))\n",
    "    for step, (image_embedding, gt2D, boxes, slice_names, label_nums) in enumerate(tqdm(dataloader)):\n",
    "        image_embedding, gt2D, boxes = image_embedding.to(device), gt2D.to(device), boxes.to(device)\n",
    "        medsam_pred = torch.as_tensor(medsam_inference(\n",
    "            model, image_embedding, boxes, 256, 256,\n",
    "            as_one_hot=True, model_trained_on_multi_label=False,\n",
    "            num_classes=1), dtype=torch.uint8).to(device)\n",
    "        # hopefully this is B, 1, H, W as a binary 0/1 tensor\n",
    "        pred_collector.append(medsam_pred)\n",
    "        label_num_collector.append(label_nums)\n",
    "\n",
    "        slice_nums_list = [int(x.split('_slice')[1].split('.npy')[0]) for x in slice_names]\n",
    "        slice_collector.extend(slice_nums_list)\n",
    "    print('done all')\n",
    "    print(pred_collector[0].shape)\n",
    "    print(label_num_collector[0].shape)\n",
    "    total_tensor = torch.cat(pred_collector, dim=0)[:,0,:,:] # now should be N, H, W\n",
    "    total_labels = torch.cat(label_num_collector, dim=0)\n",
    "    for i in range(total_tensor.shape[0]):\n",
    "        this_label_num = total_labels[i].item()\n",
    "        if np.isnan(this_label_num):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        this_slice_num = slice_collector[i]\n",
    "        where_mask = torch.where(total_tensor[this_slice_num,:,:] == 1)\n",
    "        overall_tensor[ [this_slice_num] * where_mask[0].shape[0], where_mask[0], where_mask[1]] = this_label_num\n",
    "        if this_label_num not in viz:\n",
    "            viz[this_label_num] = True\n",
    "            print(f'hello at {this_label_num} {torch.where(total_tensor[this_slice_num,:,:] == 1)[0].shape}')\n",
    "    \n",
    "    \n",
    "    all_np = overall_tensor.cpu().detach().numpy()\n",
    "    #assert all_np.shape == (len(dataset), len(files), 256, 256)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    np.save(save_path + '/singletask_seg_all.npy', all_np)\n",
    "\n",
    "    return all_np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/pooled_labels_ckpt_continue_8-22-23/model_best_20230822-115028.pth'\n",
    "\n",
    "xd = dataset_to_predictions_for_pooled_model(id_list=[162935], \n",
    "                                            checkpoint = checkpoint, \n",
    "                                            save_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/pooled_seg_gifs/162935',\n",
    "                                            device='cuda',\n",
    "                                            is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 16756038\n",
      "12.0 3926\n",
      "24.0 2564\n",
      "45.0 120\n",
      "47.0 133\n",
      "78.0 800\n",
      "84.0 1548\n",
      "98.0 937\n",
      "99.0 2377\n",
      "100.0 2913\n",
      "102.0 5860\n"
     ]
    }
   ],
   "source": [
    "for xz in np.unique(xd):\n",
    "    print(xz, (xd==xz).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  12.,  24.,  45.,  47.,  78.,  84.,  98.,  99., 100., 102.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  12.,  24.,  45.,  47.,  78.,  84.,  98.,  99., 100., 102.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd = np.load('/gpfs/data/luilab/karthik/pediatric_seg_proj/pooled_seg_gifs/162935/singletask_seg_all.npy')\n",
    "np.unique(xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a val image: 151425\n",
    "xd2 = dataset_to_predictions_for_singletask_models(id_list=[151425], \n",
    "                                            checkpoint_folder_pattern = checkpoint_folder_pattern, \n",
    "                                            dataset = dataset,\n",
    "                                            save_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/saved_round2_segmentations/151425',\n",
    "                                            device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15276"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xd.cpu().detach().numpy() == 46)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
