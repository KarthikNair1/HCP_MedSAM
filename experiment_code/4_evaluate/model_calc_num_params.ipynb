{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "import random\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import wandb\n",
    "import re\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import statannot\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MedSAM_HCP.utils_hcp import *\n",
    "from MedSAM_HCP.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, model_path, num_classes):\n",
    "    result = torch.load(model_path)\n",
    "    try:\n",
    "        if 'model' in result.keys():\n",
    "            splits = model_path.split('/')\n",
    "            new_path = os.path.join('/'.join(splits[:-1]), f'{splits[-1].split(\".pth\")[0]}_sam_readable.pth')\n",
    "            print(f'model path converted to sam readable format and saved to {new_path}')\n",
    "\n",
    "            result = result['model']\n",
    "\n",
    "            # now remove the \"module.\" prefix\n",
    "            result_dict = {}\n",
    "            for k,v in result.items():\n",
    "                key_splits = k.split('.')\n",
    "                assert key_splits[0] == 'module'\n",
    "                new_k = '.'.join(key_splits[1:])\n",
    "                result_dict[new_k] = v\n",
    "\n",
    "            torch.save(result_dict, new_path)\n",
    "            model_path = new_path\n",
    "\n",
    "    except (AttributeError):\n",
    "        # already in the correct format\n",
    "        print('model path in sam readable format already')\n",
    "\n",
    "    if model_type == 'multitask_unprompted':\n",
    "        model = build_sam_vit_b_multiclass(num_classes, checkpoint=model_path).to('cuda')\n",
    "    elif model_type == 'pooltask_yolov7_prompted':\n",
    "        model = build_sam_vit_b_multiclass(num_classes, checkpoint=model_path).to('cuda')\n",
    "    else:\n",
    "        # singletask model\n",
    "        model = build_sam_vit_b_multiclass(3, checkpoint=model_path).to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "def load_model_from_label_and_type(model_type, label):\n",
    "    assert model_type in ['singletask_unprompted', 'multitask_unprompted',\n",
    "                'singletask_medsam_prompted', 'singletask_yolov7_prompted',\n",
    "                'singletask_yolov7_longer_prompted', 'pooltask_yolov7_prompted']\n",
    "    \n",
    "    if model_type == 'singletask_unprompted':\n",
    "        raise NotImplementedError\n",
    "    elif model_type == 'multitask_unprompted':\n",
    "        model_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/ce_only_resume_training_from_checkpoint_8-9-23/MedSAM_finetune_hcp_ya_constant_bbox_all_tasks-20230810-115803/medsam_model_best.pth'\n",
    "        num_classes = 103\n",
    "    elif model_type == 'singletask_medsam_prompted':\n",
    "        raise NotImplementedError\n",
    "    elif model_type == 'singletask_yolov7_prompted':\n",
    "        model_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/second_round_w_bbox_yolov7_finetunes_longer_8-17-23/label{label}/*/medsam_model_best.pth'\n",
    "        listo = glob(model_path)\n",
    "        assert len(listo) == 1\n",
    "        model_path = listo[0]\n",
    "        num_classes = 3 # note we have to pass in 3 so that we get the singletask sam model, which predicts 3 masks, even though the more accurate number would be 2\n",
    "    elif model_type == 'singletask_yolov7_longer_prompted':\n",
    "        model_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/second_round_w_bbox_yolov7_finetunes_60epochs_8-20-23/label{label}/*/medsam_model_best.pth'\n",
    "        listo = glob(model_path)\n",
    "        assert len(listo) == 1\n",
    "        model_path = listo[0]\n",
    "        num_classes = 3\n",
    "    elif model_type == 'pooltask_yolov7_prompted':\n",
    "        model_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/pooled_labels_ckpt_continue_8-22-23/model_best_20230822-115028.pth'\n",
    "        num_classes = 103 # have to pass in 103 here unfortunately because this model was accidentally trained to output 103 masks, even though only the first one is actually used and loss-propagated through\n",
    "\n",
    "    return load_model(model_type, model_path, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_trainable_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'pooltask_yolov7_prompted'\n",
    "label = 1\n",
    "model = load_model_from_label_and_type(model_type, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090440 14400224 3690216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# all mask decoder trainable parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.mask_decoder.parameters())\n",
    "used_params = get_num_trainable_params(model.mask_decoder)\n",
    "\n",
    "# subtract out \"fake\" trainable parameters\n",
    "unused_params = get_num_trainable_params(model.mask_decoder.output_hypernetworks_mlps[1:])\n",
    "print(used_params, unused_params, used_params - unused_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path converted to sam readable format and saved to /gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/second_round_w_bbox_yolov7_finetunes_60epochs_8-20-23/label1/MedSAM_finetune_final_round-20230821-200628/medsam_model_best_sam_readable.pth\n"
     ]
    }
   ],
   "source": [
    "model_type = 'singletask_yolov7_longer_prompted'\n",
    "label = 1\n",
    "model = load_model_from_label_and_type(model_type, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371169432"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used = get_num_trainable_params(model.mask_decoder)\n",
    "unused = get_num_trainable_params(model.mask_decoder.output_hypernetworks_mlps[1:])\n",
    "(used - unused) * 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path converted to sam readable format and saved to /gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/ce_only_resume_training_from_checkpoint_8-9-23/MedSAM_finetune_hcp_ya_constant_bbox_all_tasks-20230810-115803/medsam_model_best_sam_readable.pth\n"
     ]
    }
   ],
   "source": [
    "model_type = 'multitask_unprompted'\n",
    "label = 1\n",
    "model = load_model_from_label_and_type(model_type, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(104, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-103): 104 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=104, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18090440\n"
     ]
    }
   ],
   "source": [
    "used = get_num_trainable_params(model.mask_decoder)\n",
    "print(used)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
