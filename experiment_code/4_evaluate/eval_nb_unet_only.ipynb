{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/kn2347/.conda/envs/medsam/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "import random\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import wandb\n",
    "import re\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import statannot\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('../../modified_medsam_repo')\n",
    "from MedSAM_HCP.utils_hcp import *\n",
    "from MedSAM_HCP.dataset import *\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model_from_label_and_type('singletask_unet', 1)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--df_starting_mapping_path', type=str, default = '/gpfs/home/kn2347/HCP_MedSAM_project/modified_medsam_repo/hcp_mapping_processed.csv', help = 'Path to dataframe holding the integer labels in the segmentation numpy files and the corresponding text label, prior to subsetting for only the labels we are interested in.')\n",
    "parser.add_argument('--df_desired_path', type=str, default = '/gpfs/home/kn2347/HCP_MedSAM_project/modified_medsam_repo/darts_name_class_mapping_processed.csv')\n",
    "\n",
    "args = {'df_starting_mapping_path': '/gpfs/home/kn2347/HCP_MedSAM_project/modified_medsam_repo/hcp_mapping_processed.csv',\n",
    "        'df_desired_path': '/gpfs/home/kn2347/HCP_MedSAM_project/modified_medsam_repo/darts_name_class_mapping_processed.csv',\n",
    "        'train_test_splits': '/gpfs/data/luilab/karthik/pediatric_seg_proj/train_val_test_split.pickle',\n",
    "        'world_size': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.36936936936937"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11807 / 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>slice</th>\n",
       "      <th>img_slice_path</th>\n",
       "      <th>segmentation_slice_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95792</th>\n",
       "      <td>0</td>\n",
       "      <td>102109</td>\n",
       "      <td>78</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95793</th>\n",
       "      <td>1</td>\n",
       "      <td>102109</td>\n",
       "      <td>79</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95794</th>\n",
       "      <td>2</td>\n",
       "      <td>102109</td>\n",
       "      <td>80</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95795</th>\n",
       "      <td>3</td>\n",
       "      <td>102109</td>\n",
       "      <td>81</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95796</th>\n",
       "      <td>4</td>\n",
       "      <td>102109</td>\n",
       "      <td>82</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107594</th>\n",
       "      <td>11802</td>\n",
       "      <td>994273</td>\n",
       "      <td>182</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107595</th>\n",
       "      <td>11803</td>\n",
       "      <td>994273</td>\n",
       "      <td>183</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107596</th>\n",
       "      <td>11804</td>\n",
       "      <td>994273</td>\n",
       "      <td>184</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107597</th>\n",
       "      <td>11805</td>\n",
       "      <td>994273</td>\n",
       "      <td>185</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107598</th>\n",
       "      <td>11806</td>\n",
       "      <td>994273</td>\n",
       "      <td>186</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...</td>\n",
       "      <td>/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11807 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      id  slice  \\\n",
       "95792            0  102109     78   \n",
       "95793            1  102109     79   \n",
       "95794            2  102109     80   \n",
       "95795            3  102109     81   \n",
       "95796            4  102109     82   \n",
       "...            ...     ...    ...   \n",
       "107594       11802  994273    182   \n",
       "107595       11803  994273    183   \n",
       "107596       11804  994273    184   \n",
       "107597       11805  994273    185   \n",
       "107598       11806  994273    186   \n",
       "\n",
       "                                           img_slice_path  \\\n",
       "95792   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "95793   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "95794   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "95795   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "95796   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "...                                                   ...   \n",
       "107594  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "107595  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "107596  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "107597  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "107598  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/di...   \n",
       "\n",
       "                                  segmentation_slice_path  \n",
       "95792   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "95793   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "95794   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "95795   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "95796   /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "...                                                   ...  \n",
       "107594  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "107595  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "107596  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "107597  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "107598  /gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/se...  \n",
       "\n",
       "[11807 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/baseline_unet/all_labels_df.csv')\n",
    "xx = xx[xx['id'].isin(xd)]\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xd = pd.read_pickle('/gpfs/data/luilab/karthik/pediatric_seg_proj/train_val_test_split.pickle')['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11807"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path in readable format already\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_label_and_type('singletask_unet', 1)\n",
    "df_hcp, df_desired, NUM_CLASSES, label_converter, dataset = load_data_from_label_and_type('singletask_unet', 1, 'val', args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 738/738 [17:33<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "collector = run_model_over_dataset(model, dataset, 'singletask_unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dice_sensitivity': tensor([0.9382]),\n",
       " 'dice_specificity': tensor([0.8226]),\n",
       " 'overall_dice': tensor([0.9376]),\n",
       " 'label_numbers': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, model_path, num_classes):\n",
    "    result = torch.load(model_path)\n",
    "    try:\n",
    "        if 'model' in result.keys():\n",
    "            splits = model_path.split('/')\n",
    "            new_path = os.path.join('/'.join(splits[:-1]), f'{splits[-1].split(\".pth\")[0]}_sam_readable.pth')\n",
    "            print(f'model path converted to sam readable format and saved to {new_path}')\n",
    "\n",
    "            result = result['model']\n",
    "\n",
    "            # now remove the \"module.\" prefix\n",
    "            result_dict = {}\n",
    "            for k,v in result.items():\n",
    "                key_splits = k.split('.')\n",
    "                assert key_splits[0] == 'module'\n",
    "                new_k = '.'.join(key_splits[1:])\n",
    "                result_dict[new_k] = v\n",
    "\n",
    "            torch.save(result_dict, new_path)\n",
    "            model_path = new_path\n",
    "\n",
    "    except (AttributeError):\n",
    "        # already in the correct format\n",
    "        print('model path in readable format already')\n",
    "\n",
    "    if model_type == 'multitask_unprompted':\n",
    "        model = build_sam_vit_b_multiclass(num_classes, checkpoint=model_path).to('cuda')\n",
    "    elif model_type == 'pooltask_yolov7_prompted':\n",
    "        model = build_sam_vit_b_multiclass(num_classes, checkpoint=model_path).to('cuda')\n",
    "    elif model_type == 'singletask_unet':\n",
    "        model = torch.load(model_path)\n",
    "    else:\n",
    "        # singletask model\n",
    "        model = build_sam_vit_b_multiclass(3, checkpoint=model_path).to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "def load_model_from_label_and_type(model_type, label):\n",
    "    assert model_type in ['singletask_unprompted', 'multitask_unprompted',\n",
    "                'singletask_medsam_prompted', 'singletask_yolov7_prompted',\n",
    "                'singletask_yolov7_longer_prompted', 'pooltask_yolov7_prompted',\n",
    "                'singletask_unet']\n",
    "    \n",
    "    if model_type == 'singletask_unprompted':\n",
    "        raise NotImplementedError\n",
    "    elif model_type == 'multitask_unprompted':\n",
    "        model_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/ce_only_resume_training_from_checkpoint_8-9-23/MedSAM_finetune_hcp_ya_constant_bbox_all_tasks-20230810-115803/medsam_model_best.pth'\n",
    "        num_classes = 103\n",
    "    elif model_type == 'singletask_medsam_prompted':\n",
    "        raise NotImplementedError\n",
    "    elif model_type == 'singletask_yolov7_prompted':\n",
    "        model_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/second_round_w_bbox_yolov7_finetunes_longer_8-17-23/label{label}/*/medsam_model_best.pth'\n",
    "        listo = glob(model_path)\n",
    "        assert len(listo) == 1\n",
    "        model_path = listo[0]\n",
    "        num_classes = 3 # note we have to pass in 3 so that we get the singletask sam model, which predicts 3 masks, even though the more accurate number would be 2\n",
    "    elif model_type == 'singletask_yolov7_longer_prompted':\n",
    "        model_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/second_round_w_bbox_yolov7_finetunes_60epochs_8-20-23/label{label}/*/medsam_model_best.pth'\n",
    "        listo = glob(model_path)\n",
    "        assert len(listo) == 1\n",
    "        model_path = listo[0]\n",
    "        num_classes = 3\n",
    "    elif model_type == 'pooltask_yolov7_prompted':\n",
    "        model_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/pooled_labels_ckpt_continue_8-22-23/model_best_20230822-115028.pth'\n",
    "        num_classes = 103 # have to pass in 103 here unfortunately because this model was accidentally trained to output 103 masks, even though only the first one is actually used and loss-propagated through\n",
    "    elif model_type == 'singletask_unet':\n",
    "        model_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/unet_singletask_testing_5-26-24/logs_training/fifth_pass/singletask_unet-label{label}-*.pth'\n",
    "        listo = glob(model_path)\n",
    "        assert len(listo) == 1\n",
    "        model_path = listo[0]\n",
    "        num_classes = 1\n",
    "\n",
    "    return load_model(model_type, model_path, num_classes)\n",
    "def load_data_from_label_and_type(model_type, label, tag, args):\n",
    "    # e.g. tag = 'val' or 'test'\n",
    "    df_hcp = pd.read_csv(args['df_starting_mapping_path'])\n",
    "    if model_type in ['multitask_unprompted', 'pooltask_yolov7_prompted', 'singletask_unet']:\n",
    "        df_desired = pd.read_csv(args['df_desired_path'])\n",
    "    else:\n",
    "        df_desired = pd.read_csv(f'/gpfs/home/kn2347/MedSAM/class_mappings/label{label}_only_name_class_mapping.csv')\n",
    "    NUM_CLASSES = len(df_desired)\n",
    "    if model_type == 'singletask_unet':\n",
    "        NUM_CLASSES = 2\n",
    "    label_converter = LabelConverter(df_hcp, df_desired)\n",
    "\n",
    "    # train val test split\n",
    "    train_test_splits_path = args['train_test_splits']\n",
    "    dicto = pickle.load(open(train_test_splits_path, 'rb'))\n",
    "    ids = dicto[tag] # selects val or test ids, this should be a list\n",
    "\n",
    "    if args['world_size'] is not None:\n",
    "        assert args.node_rank is not None\n",
    "\n",
    "        total_len = len(ids)\n",
    "        my_start = int((args.node_rank / args.world_size) * total_len)\n",
    "        my_end = int(((args.node_rank + 1) / args.world_size) * total_len)\n",
    "        if args.node_rank == args.world_size - 1:\n",
    "            assert my_end == total_len\n",
    "        \n",
    "        ids = ids[my_start:my_end]\n",
    "        print(f'only operating on indices {my_start} to {my_end - 1} inclusive')\n",
    "    \n",
    "    df = None\n",
    "    label_id = None\n",
    "    pool_labels = None\n",
    "\n",
    "\n",
    "    # now, load data\n",
    "    if model_type in ['multitask_unprompted']:\n",
    "        # multi task\n",
    "        df = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv')\n",
    "        label_id = None\n",
    "        pool_labels = False\n",
    "\n",
    "    elif model_type in ['pooltask_yolov7_prompted']:\n",
    "        # pool task\n",
    "        df_all_samples = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv')\n",
    "        df_all_samples = df_all_samples[df_all_samples['id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "        # we should replicate all rows with the column label_number ranging from 1...102\n",
    "        labels_nums = list(range(1,103))\n",
    "        df_all_samples = pd.concat([df_all_samples]*len(labels_nums),keys = labels_nums, names = ['label_number']).reset_index(level=0)\n",
    "        if tag == 'val':\n",
    "            df_box_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/path_df_pooled_labels_only_with_bbox_yolov7.csv'\n",
    "        elif tag == 'test':\n",
    "            df_box_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/test/path_df_pooled_labels_only_with_bbox_yolov7_TEST.csv'\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        df_boxes = pd.read_csv(df_box_path,\n",
    "                               index_col=0)\n",
    "        df_boxes = df_boxes[df_boxes['id'].isin(ids)].reset_index(drop=True)\n",
    "        \n",
    "        df_all_samples = df_all_samples.drop(columns = ['bbox_0', 'bbox_1', 'bbox_2', 'bbox_3'])\n",
    "        df = df_all_samples.merge(df_boxes, how='left', on=['id','slice','image_embedding_slice_path', 'segmentation_slice_path', 'image_path', 'label_number'])\n",
    "        label_id = 1\n",
    "        pool_labels = True\n",
    "\n",
    "    elif model_type in ['singletask_unprompted']:\n",
    "        # single task unprompted\n",
    "        df = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv')\n",
    "        label_id = label\n",
    "        pool_labels = False\n",
    "\n",
    "    elif model_type in ['singletask_medsam_prompted', 'singletask_yolov7_prompted',\n",
    "                'singletask_yolov7_longer_prompted']:\n",
    "        # single task prompted\n",
    "        if model_type == 'singletask_medsam_prompted':\n",
    "            this_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/medsam/path_df_label{label}_only_with_bbox.csv'\n",
    "        elif model_type in ['singletask_yolov7_prompted', 'singletask_yolov7_longer_prompted']:\n",
    "            if tag == 'val':\n",
    "                this_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/path_df_label{label}_only_with_bbox_yolov7.csv'\n",
    "            elif tag == 'test':\n",
    "                this_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/test/path_df_label{label}_only_with_bbox_yolov7_TEST.csv'\n",
    "        \n",
    "        df_all_samples = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv')\n",
    "        df_all_samples = df_all_samples[df_all_samples['id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "        df_boxes = pd.read_csv(this_path)\n",
    "        df_boxes = df_boxes[df_boxes['id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "        df_all_samples = df_all_samples.drop(columns = ['bbox_0', 'bbox_1', 'bbox_2', 'bbox_3'])\n",
    "        df = df_all_samples.merge(df_boxes, how='left', on=['id','slice','image_embedding_slice_path', 'segmentation_slice_path', 'image_path'])\n",
    "\n",
    "        label_id = 1\n",
    "        pool_labels = False\n",
    "    \n",
    "    elif model_type in ['singletask_unet']:\n",
    "        df = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/baseline_unet/all_labels_df.csv')\n",
    "        label_id = label\n",
    "        pool_labels = False\n",
    "\n",
    "    \n",
    "    df = df[df['id'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    if model_type =='pooltask_yolov7_prompted':\n",
    "        dataset = MRIDatasetForPooled(df, label_id = label_id, bbox_shift=0, label_converter = label_converter, NUM_CLASSES=NUM_CLASSES, as_one_hot=True, pool_labels=pool_labels)\n",
    "    elif model_type == 'singletask_unet':\n",
    "        preprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n",
    "        dataset = MRIDataset_Imgs(df, label_id = label_id, bbox_shift=0, label_converter = label_converter, NUM_CLASSES=NUM_CLASSES, as_one_hot=True, pool_labels=pool_labels, preprocess_fn=preprocess_input)\n",
    "    else:\n",
    "        dataset = MRIDataset(df, label_id = label_id, bbox_shift=0, label_converter = label_converter, NUM_CLASSES=NUM_CLASSES, as_one_hot=True, pool_labels=pool_labels)\n",
    "    \n",
    "    return df_hcp, df_desired, NUM_CLASSES, label_converter, dataset\n",
    "\n",
    "def run_model_over_dataset(model, dataset, model_type):\n",
    "    batch_sz = 16   \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_sz,\n",
    "        shuffle = False,\n",
    "        num_workers = 0,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    num_classes = 1\n",
    "    if model_type=='multitask_unprompted':\n",
    "        num_classes = 103\n",
    "    collector = {'dice_sensitivity':[], 'dice_specificity':[], 'overall_dice':[], 'label_numbers':[]}\n",
    "    for step, tup in enumerate(tqdm(dataloader)):\n",
    "        if isinstance(dataset, MRIDatasetForPooled):\n",
    "            image_embedding, gt2D, boxes, slice_names, label_nums = tup\n",
    "        elif isinstance(dataset, MRIDataset_Imgs):\n",
    "            image_embedding, gt2D = tup # \"image_embedding\" here is really just the tensor of the raw image since unet does not do pre-embedding\n",
    "        else:\n",
    "            image_embedding, gt2D, boxes, slice_names = tup\n",
    "        \n",
    "        image_embedding, gt2D = image_embedding.cuda(), gt2D.cuda()\n",
    "        if model_type == 'singletask_unet':\n",
    "            pred = model(image_embedding).cuda()\n",
    "            pred = (pred > 0.5).to(torch.uint8)\n",
    "        else:\n",
    "            boxes = boxes.cuda()\n",
    "            pred = torch.as_tensor(\n",
    "                medsam_inference(model, image_embedding, boxes, 256, 256, as_one_hot=True,\n",
    "                model_trained_on_multi_label=(model_type=='multitask_unprompted'), num_classes = num_classes),\n",
    "                dtype=torch.uint8\n",
    "            ).cuda()\n",
    "\n",
    "\n",
    "        if model_type == 'multitask_unprompted':\n",
    "            assert len(pred.shape) == 4 and pred.shape[1] == 103 # (B,C,H,W)\n",
    "            assert len(gt2D.shape) == 4 and gt2D.shape[1] == 103\n",
    "        else:\n",
    "            assert len(pred.shape) == 4 and pred.shape[1] == 1 # (B, C, H, W)\n",
    "            assert len(gt2D.shape) == 4 and gt2D.shape[1] == 1\n",
    "        \n",
    "        dices_no_mask = dice_scores_multi_class(pred, gt2D, eps=1e-6, mask_empty_class_images_with_nan = False)\n",
    "        collector['dice_sensitivity'].append(dice_scores_multi_class(pred, gt2D, eps=1e-6, mask_empty_class_images_with_nan = True))\n",
    "        collector['overall_dice'].append(dices_no_mask)\n",
    "        B, classes, H, W = gt2D.shape\n",
    "        gt2D_flattened = gt2D.view(B, classes, -1)\n",
    "        is_negative_examples = (gt2D_flattened == 0).all(dim=2) # size (B,C)\n",
    "\n",
    "\n",
    "        r, c = torch.where(is_negative_examples)\n",
    "        r = r.cpu()\n",
    "        c = c.cpu()\n",
    "        res = torch.full((B, classes), torch.nan)\n",
    "        res[r, c] = dices_no_mask[r, c]\n",
    "        collector['dice_specificity'].append(res)\n",
    "\n",
    "        if model_type == 'pooltask_yolov7_prompted':\n",
    "            collector['label_numbers'].append(label_nums)\n",
    "        #xd = collector['dice_specificity'][-1]\n",
    "    \n",
    "    if model_type != 'pooltask_yolov7_prompted':\n",
    "        collector['overall_dice'] = torch.concat(collector['overall_dice']).nanmean(dim=0)\n",
    "        collector['dice_sensitivity'] = torch.concat(collector['dice_sensitivity']).nanmean(dim=0)\n",
    "        collector['dice_specificity'] = torch.concat(collector['dice_specificity']).nanmean(dim=0)\n",
    "    else:\n",
    "        collector['overall_dice'] = torch.concat(collector['overall_dice'])\n",
    "        collector['dice_sensitivity'] = torch.concat(collector['dice_sensitivity'])\n",
    "        collector['dice_specificity'] = torch.concat(collector['dice_specificity'])\n",
    "        collector['label_numbers'] = torch.concat(collector['label_numbers'])\n",
    "    return collector\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
