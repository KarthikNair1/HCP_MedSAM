{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import errno\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import shutil\n",
    "sys.path.append('./modified_medsam_repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listo1 = glob('/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/dir_structure_for_yolov7/train/images/*.png')\n",
    "listo2 = glob('/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/dir_structure_for_yolov7/val/images/*.png')\n",
    "listo3 = glob('/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/dir_structure_for_yolov7/test/images/*.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.5\n",
      "1\n",
      "2.5\n",
      "5\n",
      "10\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "fracs = [0.1, 0.5, 1, 2.5, 5, 10, 25, 50, 75, 100]\n",
    "\n",
    "for frac in fracs:\n",
    "    print(frac)\n",
    "    df = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv')\n",
    "    #split = pd.read_pickle('/gpfs/data/luilab/karthik/pediatric_seg_proj/train_val_test_split.pickle')\n",
    "    split = pd.read_pickle(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{frac}.pkl')\n",
    "\n",
    "    lookup = {}\n",
    "    for x in split['train']:\n",
    "        lookup[x] = 'train'\n",
    "    for x in split['val']:\n",
    "        lookup[x] = 'val'\n",
    "    for x in split['test']:\n",
    "        lookup[x] = 'test'\n",
    "\n",
    "    listos = {'train':[], 'val':[], 'test':[]}\n",
    "    for i, r in df.iterrows():\n",
    "        if r['id'] in lookup:\n",
    "            tag = lookup[r['id']]\n",
    "        else:\n",
    "            continue\n",
    "        path = f'/gpfs/data/cbi/hcp/hcp_ya/hcp_ya_slices_npy/dir_structure_for_yolov7/{tag}/images/{r[\"id\"]}_slice{r[\"slice\"]}.png'\n",
    "        listos[tag].append(path)\n",
    "    rt_path = f'/gpfs/data/luilab/karthik/pediatric_seg_proj/yolov10_txt_files/{frac}'\n",
    "    os.makedirs(rt_path)\n",
    "    for tag in ['train', 'val', 'test']:\n",
    "        targ = f'{rt_path}/{tag}.txt'\n",
    "        with open(targ, 'w') as f:\n",
    "            for line in listos[tag]:\n",
    "                f.write(f'{line}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write yaml files\n",
    "targ = '/gpfs/home/kn2347/yolov10/data'\n",
    "src = '/gpfs/data/luilab/karthik/pediatric_seg_proj/yolov10_txt_files'\n",
    "\n",
    "for frac in [0.1, 0.5, 1, 2.5, 5, 10, 25, 50, 75, 100]:\n",
    "    to_dup_file = f'{targ}/hcp.yaml'\n",
    "    write_file = f'{targ}/hcp_subset_{frac}.yaml'\n",
    "\n",
    "    shutil.copy(to_dup_file, write_file)\n",
    "    \n",
    "    dicto = {}\n",
    "    for tag in ['train', 'val', 'test']:\n",
    "        dicto[tag] = f'{src}/{frac}/{tag}.txt'\n",
    "    \n",
    "\n",
    "    with open(write_file, 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        newdata = file.readlines()\n",
    "    \n",
    "    # edit the lines\n",
    "    newdata[1] = f'train: /gpfs/data/luilab/karthik/pediatric_seg_proj/yolov10_txt_files/{frac}/train.txt\\n'\n",
    "    newdata[2] = f'val: /gpfs/data/luilab/karthik/pediatric_seg_proj/yolov10_txt_files/{frac}/val.txt\\n'\n",
    "    newdata[3] = f'test: /gpfs/data/luilab/karthik/pediatric_seg_proj/yolov10_txt_files/{frac}/test.txt\\n'\n",
    "\n",
    "    # write it back\n",
    "    with open(write_file, 'w') as file:\n",
    "        file.writelines(newdata )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = glob('/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/*.csv')\n",
    "for x in filelist:\n",
    "    fileid = os.path.basename(x).split('.csv')[0]\n",
    "    to_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/subsample_symlinks', fileid)\n",
    "    if not os.path.exists(os.path.join(to_path, 'train', 'images')):\n",
    "        os.makedirs(os.path.join(to_path, 'train', 'images'))\n",
    "        os.makedirs(os.path.join(to_path, 'train', 'labels'))\n",
    "\n",
    "    df_read = pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{fileid}.csv')\n",
    "    df_read = df_read.drop_duplicates(subset = ['id', 'slice'])\n",
    "    print(f'on {fileid}')\n",
    "    for idx, r in tqdm(df_read.iterrows(), total = df_read.shape[0]):\n",
    "        ending = f'{r.id}_slice{r.slice}.png'\n",
    "        from_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/train/images', ending)\n",
    "        to_path_im = os.path.join(to_path, 'train', 'images', ending)\n",
    "        try:\n",
    "            os.symlink(from_path, to_path_im)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                os.remove(to_path_im)\n",
    "                os.symlink(from_path, to_path_im)\n",
    "            else:\n",
    "                print('uncaught error')\n",
    "\n",
    "        # label\n",
    "        ending = f'{r.id}_slice{r.slice}.txt'\n",
    "        from_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/train/labels', ending)\n",
    "        to_path_la = os.path.join(to_path, 'train', 'labels', ending)\n",
    "        try:\n",
    "            os.symlink(from_path, to_path_la)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                os.remove(to_path_la)\n",
    "                os.symlink(from_path, to_path_la)\n",
    "            else:\n",
    "                print('uncaught error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78117/78117 [00:04<00:00, 15778.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 500: 78117\n",
      "on 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28133/28133 [00:01<00:00, 15050.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 100: 28133\n",
      "on 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109292/109292 [00:10<00:00, 10611.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 750: 109292\n",
      "on 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46825/46825 [00:06<00:00, 7683.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 250: 46825\n",
      "on 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21723/21723 [00:01<00:00, 12456.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 50: 21723\n",
      "on 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16121/16121 [00:00<00:00, 17423.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 5: 16121\n",
      "on 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16663/16663 [00:01<00:00, 14727.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 10: 16663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filelist = glob('/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/*.csv')\n",
    "\n",
    "\n",
    "for x in filelist:\n",
    "    listo_paths = []\n",
    "    fileid = os.path.basename(x).split('.csv')[0]\n",
    "    #to_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/subsample_symlinks', fileid)\n",
    "\n",
    "    df_read = pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{fileid}.csv')\n",
    "    df_read = df_read.drop_duplicates(subset = ['id', 'slice'])\n",
    "    print(f'on {fileid}')\n",
    "    for idx, r in tqdm(df_read.iterrows(), total = df_read.shape[0]):\n",
    "        ending = f'{r.id}_slice{r.slice}.png'\n",
    "        from_path = os.path.join('/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/dir_structure_for_yolov7/train/images', ending)\n",
    "        listo_paths.append(from_path)\n",
    "\n",
    "    with open(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/yolov7_subsample_txt_files/{fileid}.txt', 'w') as fp:\n",
    "        for item in listo_paths:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "        print(f'for {fileid}: {len(listo_paths)}')\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/50.csv')['id'].to_numpy()).shape[0] - 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16120\n",
      "16121\n",
      "===\n",
      "16662\n",
      "16663\n",
      "===\n",
      "21722\n",
      "21723\n",
      "===\n",
      "28132\n",
      "28133\n",
      "===\n",
      "46824\n",
      "46825\n",
      "===\n",
      "78116\n",
      "78117\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for ext in ['5', '10', '50', '100', '250', '500']:\n",
    "    print(pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/yolov7_subsample_txt_files/{ext}.txt').shape[0])\n",
    "    xd = pd.read_csv(f'/gpfs/data/luilab/karthik/pediatric_seg_proj/subset_train_id_dfs_pooled/{ext}.csv')\n",
    "    xd = xd.drop_duplicates(subset = ['id', 'slice'])\n",
    "    print(xd.shape[0])\n",
    "    #print(xd)\n",
    "    print('===')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
