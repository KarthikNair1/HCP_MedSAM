{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "import random\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import wandb\n",
    "import re\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import statannot\n",
    "\n",
    "from MedSAM_HCP.utils_hcp import *\n",
    "from MedSAM_HCP.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/pooled_labels_8-20-23/MedSAM_finetune_hcp_ya_second_round_with_bbox-20230820-215516/medsam_model_best.pth'\n",
    "convert_medsam_checkpt_to_readable_for_sam(model_path, to_save_dir = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/pooled_labels_8-20-23/model_best_20230820-215516.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcp = pd.read_csv('/gpfs/home/kn2347/MedSAM/hcp_mapping_processed.csv')\n",
    "df_desired = pd.read_csv('/gpfs/home/kn2347/MedSAM/darts_name_class_mapping_processed.csv')\n",
    "NUM_CLASSES = len(df_desired)\n",
    "label_converter = LabelConverter(df_hcp, df_desired)\n",
    "\n",
    "\n",
    "checkpoint = '/gpfs/data/luilab/karthik/pediatric_seg_proj/results_copied_from_kn2347/pooled_labels_8-20-23/model_best_20230820-215516.pth'\n",
    "device = 'cuda:0'\n",
    "sam_model = build_sam_vit_b_multiclass(NUM_CLASSES, checkpoint=checkpoint).to(device)\n",
    "sam_model.eval()\n",
    "\n",
    "path_df_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/per_class_isolated_df/yolov7/path_df_pooled_labels_only_with_bbox_yolov7.csv'\n",
    "train_test_splits_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/train_val_test_split.pickle'\n",
    "train_dataset, val_dataset, test_dataset = load_datasets(path_df_path, train_test_splits_path, label_id = None, bbox_shift=0, sample_n_slices = None, label_converter= label_converter, NUM_CLASSES=NUM_CLASSES, as_one_hot=True, pool_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6132 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.78 GiB total capacity; 13.69 GiB already allocated; 128.00 MiB free; 14.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m label_numbers \u001b[39m=\u001b[39m lab_nums[batch_sz\u001b[39m*\u001b[39mstep:batch_sz\u001b[39m*\u001b[39m(step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[1;32m     16\u001b[0m image_embedding, gt2D, boxes \u001b[39m=\u001b[39m image_embedding\u001b[39m.\u001b[39mcuda(), gt2D\u001b[39m.\u001b[39mcuda(), boxes\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 17\u001b[0m medsam_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(medsam_inference(sam_model, image_embedding, boxes, \u001b[39m256\u001b[39;49m, \u001b[39m256\u001b[39;49m, as_one_hot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, model_trained_on_multi_label\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     18\u001b[0m pred \u001b[39m=\u001b[39m medsam_pred[:,\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m,:,:] \u001b[39m# B, 103, H, W -> B, 1, H, W\u001b[39;00m\n\u001b[1;32m     21\u001b[0m subset_gt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtake_along_dim(gt2D, label_numbers[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[0;32m~/MedSAM/MedSAM_HCP/MedSAM.py:96\u001b[0m, in \u001b[0;36mmedsam_inference\u001b[0;34m(medsam_model, img_embed, box_1024, H, W, as_one_hot, model_trained_on_multi_label)\u001b[0m\n\u001b[1;32m     90\u001b[0m     box_torch \u001b[39m=\u001b[39m box_torch[:, \u001b[39mNone\u001b[39;00m, :] \u001b[39m# (B, 1, 4)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m sparse_embeddings, dense_embeddings \u001b[39m=\u001b[39m medsam_model\u001b[39m.\u001b[39mprompt_encoder(\n\u001b[1;32m     92\u001b[0m     points\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m     boxes\u001b[39m=\u001b[39mbox_torch,\n\u001b[1;32m     94\u001b[0m     masks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 96\u001b[0m low_res_logits, _ \u001b[39m=\u001b[39m medsam_model\u001b[39m.\u001b[39;49mmask_decoder(\n\u001b[1;32m     97\u001b[0m     image_embeddings\u001b[39m=\u001b[39;49mimg_embed, \u001b[39m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m     image_pe\u001b[39m=\u001b[39;49mmedsam_model\u001b[39m.\u001b[39;49mprompt_encoder\u001b[39m.\u001b[39;49mget_dense_pe(), \u001b[39m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m     99\u001b[0m     sparse_prompt_embeddings\u001b[39m=\u001b[39;49msparse_embeddings, \u001b[39m# (B, 2, 256)\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m     dense_prompt_embeddings\u001b[39m=\u001b[39;49mdense_embeddings, \u001b[39m# (B, 256, 64, 64)\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m     multimask_output\u001b[39m=\u001b[39;49m model_trained_on_multi_label,\n\u001b[1;32m    102\u001b[0m     ) \u001b[39m# output of this will be (B, C, H, W)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m convert_logits_to_preds_onehot(low_res_logits, as_one_hot, H, W)\n",
      "File \u001b[0;32m~/.conda/envs/medsam/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MedSAM/segment_anything/modeling/mask_decoder.py:96\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     75\u001b[0m     image_embeddings: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     multimask_output: \u001b[39mbool\u001b[39m,\n\u001b[1;32m     80\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m      torch.Tensor: batched predictions of mask quality\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     masks, iou_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_masks(\n\u001b[1;32m     97\u001b[0m         image_embeddings\u001b[39m=\u001b[39;49mimage_embeddings,\n\u001b[1;32m     98\u001b[0m         image_pe\u001b[39m=\u001b[39;49mimage_pe,\n\u001b[1;32m     99\u001b[0m         sparse_prompt_embeddings\u001b[39m=\u001b[39;49msparse_prompt_embeddings,\n\u001b[1;32m    100\u001b[0m         dense_prompt_embeddings\u001b[39m=\u001b[39;49mdense_prompt_embeddings,\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    103\u001b[0m     \u001b[39m# Select the correct mask or masks for output\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m multimask_output:\n",
      "File \u001b[0;32m~/MedSAM/segment_anything/modeling/mask_decoder.py:143\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m# Upscale mask embeddings and predict masks using the mask tokens\u001b[39;00m\n\u001b[1;32m    142\u001b[0m src \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mview(b, c, h, w)\n\u001b[0;32m--> 143\u001b[0m upscaled_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_upscaling(src)\n\u001b[1;32m    144\u001b[0m hyper_in_list: List[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mask_tokens):\n",
      "File \u001b[0;32m~/.conda/envs/medsam/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/medsam/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/medsam/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MedSAM/segment_anything/modeling/common.py:42\u001b[0m, in \u001b[0;36mLayerNorm2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m s \u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m u)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m x \u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m u) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msqrt(s \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n\u001b[0;32m---> 42\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight[:, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m] \u001b[39m*\u001b[39;49m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.78 GiB total capacity; 13.69 GiB already allocated; 128.00 MiB free; 14.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab_nums = torch.Tensor(val_dataset.data_frame['label_number'].to_numpy()).long().cuda()\n",
    "batch_sz = 64\n",
    "dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size = batch_sz,\n",
    "        shuffle = False,\n",
    "        num_workers = 0,\n",
    "        pin_memory = True)\n",
    "        \n",
    "class_dice_collector = []\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "for step, (image_embedding, gt2D, boxes, slice_names) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        label_numbers = lab_nums[batch_sz*step:batch_sz*(step+1)]\n",
    "        image_embedding, gt2D, boxes = image_embedding.cuda(), gt2D.cuda(), boxes.cuda()\n",
    "        medsam_pred = torch.as_tensor(medsam_inference(sam_model, image_embedding, boxes, 256, 256, as_one_hot=True, model_trained_on_multi_label=False), dtype=torch.uint8).cuda()\n",
    "        pred = medsam_pred[:,0:1,:,:] # B, 103, H, W -> B, 1, H, W\n",
    "        \n",
    "\n",
    "        subset_gt = torch.take_along_dim(gt2D, label_numbers[:, None, None, None], dim=1).long()\n",
    "        dice_scores = dice_scores_multi_class(pred, subset_gt, eps=1e-6) # (B, C)\n",
    "        class_dice_collector.append(dice_scores)\n",
    "\n",
    "\n",
    "#medsam_inference(medsam_model, img_embed, box_1024, H, W, as_one_hot, model_trained_on_multi_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(595188, device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
