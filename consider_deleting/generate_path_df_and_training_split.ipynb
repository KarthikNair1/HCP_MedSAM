{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty old notebook from early on. It shows the training process roughly in a step-wise manner. I think I also generated the train-val-test split file\n",
    "here that I used for the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "join = os.path.join\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from monai.networks import one_hot\n",
    "from segment_anything import SamPredictor, sam_model_registry, build_sam_vit_b_multiclass\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from utils.SurfaceDice import compute_dice_coefficient\n",
    "from skimage import io, transform\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "from typing import Callable\n",
    "# set seeds\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "\n",
    "from MedSAM_HCP.dataset import MRIDataset, load_datasets\n",
    "from MedSAM_HCP.MedSAM import MedSAM, medsam_inference\n",
    "from MedSAM_HCP.build_sam import build_sam_vit_b_multiclass\n",
    "from MedSAM_HCP.utils_hcp import *\n",
    "\n",
    "# =========================================================\n",
    "# Generate path_df\n",
    "# =========================================================\n",
    "\n",
    "# requires medsam_preprocess_img.py to be run, resulting in encoded slices folder and segmentation slices folder\n",
    "# place the names of these folders below\n",
    "\n",
    "data_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/pretrained_image_encoded_slices'\n",
    "data_labels_path = '/gpfs/data/luilab/karthik/pediatric_seg_proj/hcp_ya_slices_npy/segmentation_slices'\n",
    "image_path = '/gpfs/data/cbi/hcp/hcp_seg/data_orig' # path to the original MRI files (*.mgz)\n",
    "folder_paths = sorted(glob(os.path.join(data_path, '*')))\n",
    "\n",
    "# construct dataframe with all data and slices\n",
    "# columns:\n",
    "\n",
    "# ID    Slice #      Path_To_Slice_Npy     Path_To_Slice_Segmentation_Npy     Path_To_Image\n",
    "\n",
    "# also add columns for the area of each segmentation in the slice\n",
    "\n",
    "collector_dict = {'id': [], \n",
    "                'slice': [], \n",
    "                'image_embedding_slice_path': [],\n",
    "                'segmentation_slice_path': [],\n",
    "                'image_path': []\n",
    "}\n",
    "\n",
    "NUM_CLASSES = 256\n",
    "region_areas = []\n",
    "for i, elem in enumerate(tqdm(folder_paths)):\n",
    "    id = elem.split('/')[-1]\n",
    "    seg_path = os.path.join(data_labels_path, id)\n",
    "    for slice_name in os.listdir(elem):\n",
    "        slice_id = slice_name.split('.')[0]\n",
    "        \n",
    "        data_path_this_slice = os.path.join(elem, slice_name)\n",
    "        seg_path_this_slice = os.path.join(seg_path, f'seg_{slice_name}')\n",
    "\n",
    "        path_to_overall_image = os.path.join(image_path, id, 'mri', 'T1.mgz')\n",
    "\n",
    "        collector_dict['id'].append(int(id))\n",
    "        collector_dict['slice'].append(int(slice_id))\n",
    "        collector_dict['image_embedding_slice_path'].append(data_path_this_slice)\n",
    "        collector_dict['segmentation_slice_path'].append(seg_path_this_slice)\n",
    "        collector_dict['image_path'].append(path_to_overall_image)\n",
    "\n",
    "#        seg_loaded = torch.Tensor(np.load(seg_path_this_slice)).to('cuda') # 256x256 of class indices\n",
    "#        tmp_region_area = []\n",
    "#        for class_num in range(NUM_CLASSES):\n",
    "#            tmp_region_area.append((seg_loaded == class_num).sum().cpu().item())\n",
    "#        region_areas.append(tmp_region_area)\n",
    "\n",
    "df = pd.DataFrame.from_dict(collector_dict)\n",
    "df = df.sort_values(by = ['id', 'slice'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# add the region_areas columns\n",
    "# stacked = np.vstack(region_areas)\n",
    "\n",
    "\n",
    "df.to_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df.csv', index=False)\n",
    "\n",
    "# make version of the dataframe with constant bbox (0,0,256,256) for MedSAM input\n",
    "df_constant_bbox = df\n",
    "df_constant_bbox['bbox_0'] = 0\n",
    "df_constant_bbox['bbox_1'] = 0\n",
    "df_constant_bbox['bbox_2'] = 256\n",
    "df_constant_bbox['bbox_3'] = 256\n",
    "\n",
    "df_constant_bbox.to_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv', index=False)\n",
    "\n",
    "# make version of the dataframe w/ constant bbox and discard 90% of blank images (defined as slice_num <=30 or >=225)\n",
    "df = pd.read_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox.csv')\n",
    "df_trim = df[(30 < df['slice']) & (df['slice'] < 225)]\n",
    "df_edges = df[(df['slice'] <= 30) | (225 <= df['slice'])]\n",
    "df_edges_subsampled = df_edges.sample(frac = 0.10, replace=False, random_state=182)\n",
    "\n",
    "df_total = pd.concat([df_trim, df_edges_subsampled], axis=0).reset_index()\n",
    "df_total.to_csv('/gpfs/data/luilab/karthik/pediatric_seg_proj/path_df_constant_bbox_remove_most_blank.csv', index=False)\n",
    "\n",
    "# =========================================================\n",
    "# Generate train-val-test split in ratio 80%:10%:10%\n",
    "# =========================================================\n",
    "ids = [int(x.split('/')[-1]) for x in folder_paths]\n",
    "\n",
    "# train, val, test split on the id's\n",
    "size_val = round(0.1 * len(ids))\n",
    "size_test = round(0.1 * len(ids))\n",
    "\n",
    "trainval_ids, test_ids = train_test_split(ids, test_size=size_test, random_state = 2023, shuffle=True)\n",
    "train_ids, val_ids = train_test_split(trainval_ids, test_size=size_val, random_state = 2023, shuffle=True)\n",
    "\n",
    "# save the id's to a place permanently\n",
    "\n",
    "dicto = {'train':train_ids, 'val':val_ids, 'test':test_ids}\n",
    "with open('/gpfs/data/luilab/karthik/pediatric_seg_proj/train_val_test_split.pickle', 'wb') as file:\n",
    "    pickle.dump(dicto,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
